{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1485bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import BorderlineSMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4527113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"D:\\BigData And DataMining\\Đồ án\\TH4\\Myocardial infarction complications Database.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aecdb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Xóa cột có >90% giá trị thiếu\n",
    "missing_ratio = df.isnull().mean()\n",
    "cols_to_drop = missing_ratio[missing_ratio > 0.9].index\n",
    "df.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7afceb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\impute\\_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. MICE cho các cột <30% thiếu\n",
    "cols_mice = missing_ratio[(missing_ratio > 0) & (missing_ratio <= 0.3)].index\n",
    "mice_imputer = IterativeImputer(random_state=0)\n",
    "df[cols_mice] = mice_imputer.fit_transform(df[cols_mice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad3379dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_16336\\1346696598.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# 4. mean/mode cho các cột có 30–90% thiếu\n",
    "cols_mean_mode = missing_ratio[(missing_ratio > 0.3) & (missing_ratio <= 0.9)].index\n",
    "for col in cols_mean_mode:\n",
    "    if df[col].dtype in [np.float64, np.float32]:\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "    else:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a808a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Chuẩn hóa đặc trưng\n",
    "target = 'STENOK_AN'\n",
    "X = df.drop(columns=['ID', 'STENOK_AN'])\n",
    "y = df['STENOK_AN'].astype(int)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ede35d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Cân bằng bằng Borderline-SMOTE\n",
    "sm = BorderlineSMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_scaled, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89ca72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Xuất dữ liệu đã xử lý (nếu cần)\n",
    "X_final = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "y_final = pd.Series(y_resampled, name=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2260657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier,\n",
    "    RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44877d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia dữ liệu\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42f29428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20115\n",
      "[LightGBM] [Info] Number of data points in the train set: 3707, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.981609\n",
      "[LightGBM] [Info] Start training from score -1.956487\n",
      "[LightGBM] [Info] Start training from score -1.950778\n",
      "[LightGBM] [Info] Start training from score -1.943216\n",
      "[LightGBM] [Info] Start training from score -1.915359\n",
      "[LightGBM] [Info] Start training from score -1.930120\n",
      "[LightGBM] [Info] Start training from score -1.945101\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19170\n",
      "[LightGBM] [Info] Number of data points in the train set: 2965, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.983365\n",
      "[LightGBM] [Info] Start training from score -1.954378\n",
      "[LightGBM] [Info] Start training from score -1.949627\n",
      "[LightGBM] [Info] Start training from score -1.944899\n",
      "[LightGBM] [Info] Start training from score -1.914699\n",
      "[LightGBM] [Info] Start training from score -1.930847\n",
      "[LightGBM] [Info] Start training from score -1.944899\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001709 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19057\n",
      "[LightGBM] [Info] Number of data points in the train set: 2965, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.980917\n",
      "[LightGBM] [Info] Start training from score -1.956761\n",
      "[LightGBM] [Info] Start training from score -1.951999\n",
      "[LightGBM] [Info] Start training from score -1.942543\n",
      "[LightGBM] [Info] Start training from score -1.916990\n",
      "[LightGBM] [Info] Start training from score -1.928524\n",
      "[LightGBM] [Info] Start training from score -1.944899\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19267\n",
      "[LightGBM] [Info] Number of data points in the train set: 2966, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.981254\n",
      "[LightGBM] [Info] Start training from score -1.957099\n",
      "[LightGBM] [Info] Start training from score -1.952337\n",
      "[LightGBM] [Info] Start training from score -1.942880\n",
      "[LightGBM] [Info] Start training from score -1.915036\n",
      "[LightGBM] [Info] Start training from score -1.928861\n",
      "[LightGBM] [Info] Start training from score -1.945236\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18971\n",
      "[LightGBM] [Info] Number of data points in the train set: 2966, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.981254\n",
      "[LightGBM] [Info] Start training from score -1.957099\n",
      "[LightGBM] [Info] Start training from score -1.949964\n",
      "[LightGBM] [Info] Start training from score -1.942880\n",
      "[LightGBM] [Info] Start training from score -1.915036\n",
      "[LightGBM] [Info] Start training from score -1.931184\n",
      "[LightGBM] [Info] Start training from score -1.945236\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19030\n",
      "[LightGBM] [Info] Number of data points in the train set: 2966, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.981254\n",
      "[LightGBM] [Info] Start training from score -1.957099\n",
      "[LightGBM] [Info] Start training from score -1.949964\n",
      "[LightGBM] [Info] Start training from score -1.942880\n",
      "[LightGBM] [Info] Start training from score -1.915036\n",
      "[LightGBM] [Info] Start training from score -1.931184\n",
      "[LightGBM] [Info] Start training from score -1.945236\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\training.py:183: UserWarning: [14:27:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\training.py:183: UserWarning: [14:28:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\training.py:183: UserWarning: [14:28:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\training.py:183: UserWarning: [14:28:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\training.py:183: UserWarning: [14:28:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\training.py:183: UserWarning: [14:28:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\training.py:183: UserWarning: [14:29:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\training.py:183: UserWarning: [14:29:51] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\training.py:183: UserWarning: [14:29:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\training.py:183: UserWarning: [14:30:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\training.py:183: UserWarning: [14:30:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\xgboost\\training.py:183: UserWarning: [14:30:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"Bagging\": BaggingClassifier(),\n",
    "    \"GDBT\": GradientBoostingClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"LGB\": LGBMClassifier(),\n",
    "    \"QDA\": QuadraticDiscriminantAnalysis(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"Tree\": DecisionTreeClassifier(),\n",
    "    \"XGB\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "}\n",
    "\n",
    "# Thêm Stacking và Voting riêng\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier()),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        ('svc', SVC(probability=True))\n",
    "    ],\n",
    "    final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier()),\n",
    "        ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n",
    "        ('svc', SVC(probability=True))\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "models[\"Stacking\"] = stacking_model\n",
    "models[\"Voting\"] = voting_model\n",
    "\n",
    "# Đánh giá từng mô hình\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    cv_score = cross_val_score(model, X_train, y_train, cv=5).mean()\n",
    "    results.append((name, acc, cv_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
